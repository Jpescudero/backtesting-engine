{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Investigación de barridas en NDXm\n\nEste cuaderno explora patrones de microestructura intradía y eventos de **barrida bajista** sobre NDXm. Objetivos principales:\n\n- Cargar datos OHLCV intradía desde los NPZ existentes en el pipeline.\n- Construir features microestructurales (mechas, rango, ATR, hora, volumen relativo).\n- Definir formalmente el evento de \"barrida bajista\".\n- Analizar estadísticamente los retornos posteriores a las barridas.\n- Conectar con el motor de backtesting para probar señales simples derivadas de las barridas.\n- Dejar preparado un esquema básico de separación **train/test** por años."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup e imports"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import sys, os\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\nif PROJECT_ROOT not in sys.path:\n    sys.path.append(PROJECT_ROOT)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom src.data.feeds import NPZOHLCVFeed\nfrom src.engine.core import BacktestConfig, run_backtest_with_signals\nfrom src.analytics.metrics import equity_curve_metrics, trades_metrics\nfrom src.analytics.reporting import trades_to_dataframe\n\n%matplotlib inline\nplt.style.use(\"seaborn-v0_8-darkgrid\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Carga de datos"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "SYMBOL = \"NDXm\"\nYEARS = [2020, 2021, 2022]  # ajustable\n\nfeed = NPZOHLCVFeed(SYMBOL)\nohlcv = feed.load_years(YEARS)\n\nts = pd.to_datetime(ohlcv.ts, unit=\"s\")\ndf = pd.DataFrame({\n    \"open\": ohlcv.o,\n    \"high\": ohlcv.h,\n    \"low\": ohlcv.l,\n    \"close\": ohlcv.c,\n    \"volume\": ohlcv.v,\n}, index=ts).sort_index()\n\ndf_head = df.head()\ndf_stats = df.describe()\n\ndf_head, df_stats\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Construcción de features microestructurales"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def add_microstructure_features(df: pd.DataFrame, atr_window: int = 14) -> pd.DataFrame:\n    out = df.copy()\n    out[\"range\"] = out[\"high\"] - out[\"low\"]\n    out[\"upper_wick\"] = out[\"high\"] - out[[\"open\", \"close\"]].max(axis=1)\n    out[\"lower_wick\"] = out[[\"open\", \"close\"]].min(axis=1) - out[\"low\"]\n\n    prev_close = out[\"close\"].shift(1)\n    tr_components = pd.concat([\n        out[\"high\"] - out[\"low\"],\n        (out[\"high\"] - prev_close).abs(),\n        (out[\"low\"] - prev_close).abs(),\n    ], axis=1)\n    out[\"tr\"] = tr_components.max(axis=1)\n    out[\"atr\"] = out[\"tr\"].rolling(atr_window).mean()\n\n    out[\"hour\"] = out.index.hour\n\n    for horizon in [5, 10, 20]:\n        out[f\"fwd_{horizon}m\"] = out[\"close\"].shift(-horizon) / out[\"close\"] - 1.0\n\n    return out\n\n\ndf_feat = add_microstructure_features(df)\ndf_feat.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Definición de barrida"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def compute_sweep_flag(\n    df: pd.DataFrame,\n    wick_threshold: float = 0.6,\n    vol_window: int = 20,\n    vol_quantile: float = 0.8,\n    hour_window: tuple[int, int] | None = None,\n) -> pd.Series:\n    '''\n    Marca como True las velas que consideramos 'barrida bajista':\n    - Mecha inferior grande: lower_wick > wick_threshold * range\n    - Volumen alto: volume > rolling quantile(vol_window, vol_quantile)\n    - (Opcional) restringido a una ventana horaria [h0, h1]\n    '''\n    vol_rolling = df[\"volume\"].rolling(vol_window)\n    vol_p = vol_rolling.quantile(vol_quantile)\n\n    cond_wick = df[\"lower_wick\"] > df[\"range\"] * wick_threshold\n    cond_vol = df[\"volume\"] > vol_p\n\n    if hour_window is not None:\n        h0, h1 = hour_window\n        cond_hour = (df[\"hour\"] >= h0) & (df[\"hour\"] <= h1)\n    else:\n        cond_hour = True\n\n    is_sweep = cond_wick & cond_vol & cond_hour\n    return is_sweep.fillna(False)\n\n\ndef build_long_signal_from_sweep(is_sweep: pd.Series) -> pd.Series:\n    sig = pd.Series(0, index=is_sweep.index, dtype=np.int8)\n    sig[is_sweep] = 1\n    return sig\n\n\nis_sweep = compute_sweep_flag(df_feat)\ndf_feat[\"signal_long\"] = build_long_signal_from_sweep(is_sweep)\ndf_feat[[\"range\", \"lower_wick\", \"signal_long\"]].head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Análisis estadístico de barridas"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sweep_stats = df_feat.loc[is_sweep, [\"fwd_5m\", \"fwd_10m\", \"fwd_20m\"]].describe()\nno_sweep_stats = df_feat.loc[~is_sweep, [\"fwd_5m\", \"fwd_10m\", \"fwd_20m\"]].describe()\n\nprob_up_sweep = (df_feat.loc[is_sweep, \"fwd_10m\"] > 0).mean()\nprob_up_no_sweep = (df_feat.loc[~is_sweep, \"fwd_10m\"] > 0).mean()\n\nmean_by_hour = df_feat.groupby(\"hour\")[\"fwd_10m\"].mean()\n\nprint(\"Prob. fwd_10m > 0 tras barrida:\", prob_up_sweep)\nprint(\"Prob. fwd_10m > 0 sin barrida:\", prob_up_no_sweep)\n\n(ax := mean_by_hour.plot(kind=\"bar\", figsize=(10, 4))).set_title(\"Retorno medio a 10m por hora del día\")\nplt.show()\n\nsweep_stats, no_sweep_stats\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Helper para backtest desde señal"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def run_backtest_from_signal(df_feat: pd.DataFrame, signal: pd.Series,\n                             sl_atr: float = 1.5,\n                             tp_atr: float = 3.0,\n                             max_trade_minutes: int = 60,\n                             initial_cash: float = 10_000):\n    ts_arr = df_feat.index.view(\"int64\") // 10**9\n    o_arr = df_feat[\"open\"].to_numpy()\n    h_arr = df_feat[\"high\"].to_numpy()\n    l_arr = df_feat[\"low\"].to_numpy()\n    c_arr = df_feat[\"close\"].to_numpy()\n    v_arr = df_feat[\"volume\"].to_numpy()\n    sig_arr = signal.to_numpy()\n    atr_arr = df_feat[\"atr\"].to_numpy()\n\n    cfg = BacktestConfig(\n        initial_cash=initial_cash,\n        sl_atr=sl_atr,\n        tp_atr=tp_atr,\n        max_trade_minutes=max_trade_minutes,\n        commission_per_trade=0.0,\n        slippage_ticks=0,\n    )\n\n    result = run_backtest_with_signals(\n        ts_arr, o_arr, h_arr, l_arr, c_arr, v_arr,\n        sig_arr,\n        atr_arr,\n        cfg,\n    )\n\n    metrics = {}\n    if equity_curve_metrics is not None:\n        eqm = equity_curve_metrics(result.equity_curve)\n        metrics.update(eqm.to_dict())\n\n    if trades_metrics is not None and trades_to_dataframe is not None:\n        tdf = trades_to_dataframe(result)\n        tmet = trades_metrics(tdf)\n        metrics.update({f\"trades_{k}\": v for k, v in tmet.to_dict().items()})\n\n    return result, metrics\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Ejemplo de backtest de barrida"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "is_sweep = compute_sweep_flag(df_feat, wick_threshold=0.6, vol_quantile=0.8, hour_window=(8, 10))\nsignal = build_long_signal_from_sweep(is_sweep)\nresult, metrics = run_backtest_from_signal(df_feat, signal)\n\npd.Series(metrics)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Separación train/test por años"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def split_by_years(df: pd.DataFrame, train_years, test_years):\n    train_mask = df.index.year.isin(train_years)\n    test_mask = df.index.year.isin(test_years)\n    return df[train_mask].copy(), df[test_mask].copy()\n\n\ndf_train, df_test = split_by_years(df_feat, train_years=[2020, 2021], test_years=[2022])\n\n# Ejemplo: usar df_train para calibrar parámetros y reservar df_test para validar\nlen(df_train), len(df_test)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notas y siguientes pasos\n\n- Ajustar parámetros de definición de barrida (mecha, volumen, ventana horaria) y medir estabilidad.\n- Experimentar con filtros adicionales (tendencia intradía, volatilidad previa, distancias a VWAP si está disponible).\n- Extender señales para posiciones cortas o estrategias de reversión.\n- Validar en `df_test` y documentar resultados clave."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}